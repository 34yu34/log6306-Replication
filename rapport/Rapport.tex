\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

\title{Refactorings and Technical Debt in Docker Projects: A Replication Study}

\author{Billy Bouchard, Quentin }
        % <-this % stops a space
\thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
\thanks{Manuscript received April 19, 2021; revised August 16, 2021.}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

\maketitle

\begin{abstract}
This document is intended to be a replication study of the work of \cite{1} on the refactorings and technical debt in docker projects.
\end{abstract}

\begin{IEEEkeywords}
Article submission, IEEE, IEEEtran, journal, \LaTeX, paper, template, typesetting.
\end{IEEEkeywords}

\section{Introduction}
Distributed systems have many uses, but also have many flaws, one of which being the lack of standard between the systems that you deploy on.
Docker and container try to help in that regards, focusing on giving a simple standard interface from which the operating system used matters less.
These tools lets the developper script the way a systems is supposed to work, and this idependently form the operatin system(OS).
Their images can be reused and redeployed so that you can rebuild the same project on another machine in a matter of minutes.
By their own nature, containers most often use in complicated systems and are, by that very same definition, usually hard to maintain.
Well maintained docker projects can save developper a lot of headaches and time and even gives them powerful tools.
However, poorly maintained ones can be even more frustrating than directly developping on the OS.
The research from \cite{1} try to categorize the different refactoring practices in docker systems and find those that are use the most often.
Few study before the one being replicated have study the maintenance cycle of docker projects.
Thus, the importance of this study and of the work of categorisation it has done.
This replication will challenge the methodology process and the results obtain by \cite{1}.
The main focus of this replication isn't to add more commits and projects to the pool in order to confirm the conclusion, but to look at the already analyzed data and verify the conclusions. 

\section{Methodology}

The research looked into a total of 68 open-source projects with over 19 MLOC. 
They fetch these with the help of the GitHub archive BigQuery that contained a total of 2000+ docker projects.
Their selection process skimed the project that where fork from other and the non existing ones.
Looking into keywords, any commits with REFACTOR in it has been kept.
this resulted in 4,469 projects with a maximum of 73 commits per repository, finally keeping only 68 project and 193 commits that where mostly written in java.
The commit ranged from 1 to 12 per project with a total of 611 files changed.

The replication used only 10\% of the data of the first study (around 62 file change) to verify the conclusion.
The process for the seleciton of the subdata set was initially random based. 
However, further looking into the data we decided to keep the same ratios of docker and docker-compose files.
Finally, taking the whole list of commit from the replication package, every n commit(depending on the list size) were taken into the subdata set.
This permitted to spread all the analyze data equally accross projects and changes.
A manual check was then done on all the selected subdata where the study conclusion where looked at to be either confirm or infirm.
From the manual check, 4 possible outcomes were looked at.
Either the initial conclusion was good, the change was not a refactor, the change was a refactor but from a different type or something that was assumed not to be a refactor was deduced to be one.
Once the first categorisation done in the subdata set, a second check was specifically done on the data that was refactored to see if the types of the refactorings could be expanded.
The goal of this second phase was to see if the initial conclusion of the study really considered all the possible refactors of a docker or docker-compose file.


\section{Results}

\section{Implication and Discussion}

\section{Validity}

This replication study shares validity threats with the study from \cite{1} which it replicates.
More specificly, it shares all the same issue with data gathering than the first study since the data taken was exactly the same.
However, some other threats came from the subdata selection process, which could have not been representative of the total data.

\section{Conclusion}

\section{References Section}
%
\section{Simple References}
You can manually copy in the resultant .bbl file and set second argument of $\backslash${\tt{begin}} to the number of references
 (used to reserve space for the reference number labels box).

\begin{thebibliography}{1}
\bibliographystyle{IEEEtran}

\bibitem[Ksontini et al.]{1}
E. Ksontini, M. Kessentini, T. d. N. Ferreira and F. Hassan, "Refactorings and Technical Debt in Docker Projects: An Empirical Study," \textit{2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, Melbourne, Australia, 2021, pp. 781-791, doi: 10.1109/ASE51524.2021.9678585.

\end{thebibliography}

\vfill

\end{document}

